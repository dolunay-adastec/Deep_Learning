{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car Continuous Environment Control Problem\n",
    "#### Solved by Using Reinforcement Learning with Deep Deterministic Policy Gradient\n",
    "\n",
    "References:\n",
    "- https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\n",
    "- https://github.com/Bduz/intro_pytorch/tree/main/intro_rl/ddpg\n",
    "- https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "\n",
    "**Description:**\n",
    "The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can be applied to the car in either direction. The goal of the MDP is to strategically accelerate the car to reach the goal state on top of the right hill. There are two versions of the mountain car domain in gymnasium: one with discrete actions and one with continuous. This version is the one with continuous actions.\n",
    "\n",
    "This MDP first appeared in Andrew Moore’s PhD Thesis (1990)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning my repo (for google colab)\n",
    "!git clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the path to the sys (for google colab)\n",
    "import sys\n",
    "sys.path.instert(0, \"/content/Deep_Learning/DRL_Deep_Reinforcement_Learning/DRL_DQN/DDPG_Mountain_Car_Continuous/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Gym Version 0.26.2\n",
      "Numpy Version 1.24.0\n"
     ]
    }
   ],
   "source": [
    "# Importing Necessary Libraries\n",
    "import gym\n",
    "print(\"OpenAI Gym Version\", gym.__version__)\n",
    "import numpy as np\n",
    "print(\"Numpy Version\", np.__version__)\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from DDPG_Agent_model import DDPG_Agent\n",
    "# Visıalization\n",
    "from gym import wrappers\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Initialize the Environment and the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (2,)\n",
      "Number of actions:  (1,)\n",
      "Maximum time step:  999\n",
      "Agent's State Size 2\n",
      "Agent's Action Size 1\n",
      "Agent's Random Seed None\n"
     ]
    }
   ],
   "source": [
    "# Initializing the environment\n",
    "env = gym.make('MountainCarContinuous-v0',render_mode='rgb_array')\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.shape)\n",
    "print('Maximum time step: ', env._max_episode_steps)\n",
    "\n",
    "# Initializing the agent\n",
    "# agent = DDPG_Agent(state_size=2, action_size=1,random_seed=2)\n",
    "agent = DDPG_Agent(state_size=2, action_size=1, random_seed=2)\n",
    "#agent.state_size = 2\n",
    "#agent.action_size = 1\n",
    "#agent.random_seed = 2\n",
    "print(\"Agent's State Size\", agent.state_size)\n",
    "print(\"Agent's Action Size\", agent.action_size)\n",
    "print(\"Agent's Random Seed\", agent.seed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAverage Score: -29.08\tMin Action: [0.17704758]\n",
      "Episode 40\tAverage Score: -28.03\tMin Action: [-0.01235572]\n",
      "Episode 60\tAverage Score: -23.39\tMin Action: [0.01321065]]\n",
      "Episode 80\tAverage Score: -22.67\tMin Action: [0.08813744]]\n",
      "Episode 85\tAverage Score: -22.71\tMin Action: [0.0366386]]]"
     ]
    }
   ],
   "source": [
    "from DDPG_Agent_model import DDPG_Agent\n",
    "env = gym.make('MountainCarContinuous-v0',render_mode='rgb_array')\n",
    "def ddpg(n_episodes=1000, max_t=700, print_every=20):\n",
    "    scores_deque = deque(maxlen=print_every) # last 100 scores\n",
    "    scores = [] # list containing scores from each episode\n",
    "\n",
    "    # For each episode\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()[0] # Reset the environment\n",
    "        agent.reset() # Reset the agent\n",
    "        score = 0 # Initialize the score\n",
    "        min_action = 0.5\n",
    "        #img = plt.imshow(env.render())\n",
    "        # For each time step\n",
    "        for t in range(max_t):\n",
    "            #img.set_data(env.render())\n",
    "            #plt.axis('off')\n",
    "            #display(plt.gcf())\n",
    "            #display(wait=True)\n",
    "            \n",
    "\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state # Roll over the state to next time step\n",
    "            score += reward # Update the score\n",
    "\n",
    "            if min_action > action:\n",
    "                min_action = action\n",
    "\n",
    "            #if t % 10 == 0:\n",
    "                #print(\"Action:\",action[0], \"State:\",state[0], \"Reward:\",reward, \"Score:\",score)\n",
    "                #clear_output(wait=True)\n",
    "            if done:\n",
    "                break\n",
    "            #clear_output(wait=True)\n",
    "        scores_deque.append(score) # Save most recent score to the deque\n",
    "        scores.append(score) # Save most recent score to the list\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMin Action: {}'.format(i_episode, np.mean(scores_deque), min_action), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=90.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "# Plotting the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1,len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
