{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car Continuous Environment Control Problem\n",
    "#### Solved by Using Reinforcement Learning with Deep Deterministic Policy Gradient\n",
    "\n",
    "References:\n",
    "- https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\n",
    "- https://github.com/Bduz/intro_pytorch/tree/main/intro_rl/ddpg\n",
    "- https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "\n",
    "**Description:**\n",
    "The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can be applied to the car in either direction. The goal of the MDP is to strategically accelerate the car to reach the goal state on top of the right hill. There are two versions of the mountain car domain in gymnasium: one with discrete actions and one with continuous. This version is the one with continuous actions.\n",
    "\n",
    "This MDP first appeared in Andrew Mooreâ€™s PhD Thesis (1990)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning my repo (for google colab)\n",
    "!git clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the path to the sys (for google colab)\n",
    "import sys\n",
    "sys.path.instert(0, \"/content/Deep_Learning/DRL_Deep_Reinforcement_Learning/DRL_DQN/DDPG_Mountain_Car_Continuous/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Gym Version 0.26.2\n",
      "Numpy Version 1.24.0\n"
     ]
    }
   ],
   "source": [
    "# Importing Necessary Libraries\n",
    "import gym\n",
    "print(\"OpenAI Gym Version\", gym.__version__)\n",
    "import numpy as np\n",
    "print(\"Numpy Version\", np.__version__)\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from DDPG_Agent_model import DDPG_Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Initialize the Environment and the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (2,)\n",
      "Number of actions:  (1,)\n",
      "Maximum time step:  999\n",
      "Agent's State Size 2\n",
      "Agent's Action Size 1\n",
      "Agent's Random Seed None\n"
     ]
    }
   ],
   "source": [
    "# Initializing the environment\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.shape)\n",
    "print('Maximum time step: ', env._max_episode_steps)\n",
    "\n",
    "# Initializing the agent\n",
    "# agent = DDPG_Agent(state_size=2, action_size=1,random_seed=2)\n",
    "agent = DDPG_Agent(state_size=2, action_size=1, random_seed=2)\n",
    "#agent.state_size = 2\n",
    "#agent.action_size = 1\n",
    "#agent.random_seed = 2\n",
    "print(\"Agent's State Size\", agent.state_size)\n",
    "print(\"Agent's Action Size\", agent.action_size)\n",
    "print(\"Agent's Random Seed\", agent.seed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DDPG_Agent' object has no attribute 'noise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m scores\n\u001b[1;32m---> 31\u001b[0m scores \u001b[39m=\u001b[39m ddpg()\n\u001b[0;32m     33\u001b[0m \u001b[39m# Plotting the scores\u001b[39;00m\n\u001b[0;32m     34\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure()\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mddpg\u001b[1;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i_episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_episodes\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      7\u001b[0m     state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset() \u001b[39m# Reset the environment\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     agent\u001b[39m.\u001b[39;49mreset() \u001b[39m# Reset the agent\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# Initialize the score\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39m# For each time step\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Repositories\\Personal_Repositories\\Deep_Learning\\DRL_Deep_Reinforcement_Learning\\DRL_DQN\\DDPG_Mountain_Car_Continuous\\DDPG_Agent.py:110\u001b[0m, in \u001b[0;36mDDPG_Agent.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    109\u001b[0m     \u001b[39m\"\"\"Reset the noise\"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnoise\u001b[39m.\u001b[39mreset()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DDPG_Agent' object has no attribute 'noise'"
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=1000, max_t=400, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every) # last 100 scores\n",
    "    scores = [] # list containing scores from each episode\n",
    "\n",
    "    # For each episode\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset() # Reset the environment\n",
    "        agent.reset() # Reset the agent\n",
    "        score = 0 # Initialize the score\n",
    "        \n",
    "        # For each time step\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state # Roll over the state to next time step\n",
    "            score += reward # Update the score\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(score) # Save most recent score to the deque\n",
    "        scores.append(score) # Save most recent score to the list\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=90.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "# Plotting the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1,len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
