{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car Continuous Environment Control Problem\n",
    "#### Solved by Using Reinforcement Learning with Deep Deterministic Policy Gradient\n",
    "\n",
    "References:\n",
    "- https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\n",
    "- https://github.com/Bduz/intro_pytorch/tree/main/intro_rl/ddpg\n",
    "- https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "\n",
    "**Description:**\n",
    "The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can be applied to the car in either direction. The goal of the MDP is to strategically accelerate the car to reach the goal state on top of the right hill. There are two versions of the mountain car domain in gymnasium: one with discrete actions and one with continuous. This version is the one with continuous actions.\n",
    "\n",
    "This MDP first appeared in Andrew Moore’s PhD Thesis (1990)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning my repo (for google colab)\n",
    "!git clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the path to the sys (for google colab)\n",
    "import sys\n",
    "sys.path.insert(0, \"/content/Deep_Learning/DRL_Deep_Reinforcement_Learning/DRL_DQN/DDPG_Mountain_Car_Continuous/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Gym Version 0.26.2\n",
      "Numpy Version 1.24.0\n"
     ]
    }
   ],
   "source": [
    "# Importing Necessary Libraries\n",
    "import gym\n",
    "print(\"OpenAI Gym Version\", gym.__version__)\n",
    "import numpy as np\n",
    "print(\"Numpy Version\", np.__version__)\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from DDPG_Agent_model import DDPG_Agent\n",
    "# Visıalization\n",
    "from gym import wrappers\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Initialize the Environment and the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (2,)\n",
      "Number of actions:  (1,)\n",
      "Maximum time step:  999\n",
      "Agent's State Size 2\n",
      "Agent's Action Size 1\n",
      "Agent's Random Seed None\n"
     ]
    }
   ],
   "source": [
    "# Initializing the environment\n",
    "env = gym.make('MountainCarContinuous-v0',render_mode='rgb_array')\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.shape)\n",
    "print('Maximum time step: ', env._max_episode_steps)\n",
    "\n",
    "# Initializing the agent\n",
    "# agent = DDPG_Agent(state_size=2, action_size=1,random_seed=2)\n",
    "agent = DDPG_Agent(state_size=2, action_size=1, random_seed=2)\n",
    "#agent.state_size = 2\n",
    "#agent.action_size = 1\n",
    "#agent.random_seed = 2\n",
    "print(\"Agent's State Size\", agent.state_size)\n",
    "print(\"Agent's Action Size\", agent.action_size)\n",
    "print(\"Agent's Random Seed\", agent.seed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAverage Score: -29.08\tMin Action: [0.17704758]\n",
      "Episode 40\tAverage Score: -28.03\tMin Action: [-0.01235572]\n",
      "Episode 60\tAverage Score: -23.39\tMin Action: [0.01321065]]\n",
      "Episode 80\tAverage Score: -22.67\tMin Action: [0.08813744]]\n",
      "Episode 100\tAverage Score: -22.83\tMin Action: [0.04946094]\n",
      "Episode 120\tAverage Score: -23.07\tMin Action: [-0.02671875]\n",
      "Episode 140\tAverage Score: -23.78\tMin Action: [-0.06580434]\n",
      "Episode 160\tAverage Score: -25.89\tMin Action: [-0.0261539]]\n",
      "Episode 180\tAverage Score: -23.81\tMin Action: [-0.05189868]\n",
      "Episode 200\tAverage Score: -23.24\tMin Action: [0.01884322]]\n",
      "Episode 220\tAverage Score: -27.29\tMin Action: [0.10413481]]\n",
      "Episode 227\tAverage Score: -26.52\tMin Action: [-0.08290479]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 50\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m scores\n\u001b[1;32m---> 50\u001b[0m scores \u001b[39m=\u001b[39m ddpg()\n\u001b[0;32m     52\u001b[0m \u001b[39m# Plotting the scores\u001b[39;00m\n\u001b[0;32m     53\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure()\n",
      "Cell \u001b[1;32mIn[42], line 25\u001b[0m, in \u001b[0;36mddpg\u001b[1;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[0;32m     23\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(state)\n\u001b[0;32m     24\u001b[0m next_state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m---> 25\u001b[0m agent\u001b[39m.\u001b[39;49mstep(state, action, reward, next_state, done)\n\u001b[0;32m     26\u001b[0m state \u001b[39m=\u001b[39m next_state \u001b[39m# Roll over the state to next time step\u001b[39;00m\n\u001b[0;32m     27\u001b[0m score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward \u001b[39m# Update the score\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Repositories\\Personal_Repositories\\Deep_Learning\\DRL_Deep_Reinforcement_Learning\\DRL_DQN\\DDPG_Mountain_Car_Continuous\\DDPG_Agent_model.py:86\u001b[0m, in \u001b[0;36mDDPG_Agent.step\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39m# Learn, if enough samples are available in memory\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m# If the memory size is greater than the batch size\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m BATCH_SIZE:\n\u001b[0;32m     85\u001b[0m     \u001b[39m# Sample a random batch from the replay memory\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     experiences \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msample()\n\u001b[0;32m     87\u001b[0m     \u001b[39m# Learn from the sampled batch\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn(experiences, GAMMA)\n",
      "File \u001b[1;32mc:\\Repositories\\Personal_Repositories\\Deep_Learning\\DRL_Deep_Reinforcement_Learning\\DRL_DQN\\DDPG_Mountain_Car_Continuous\\DDPG_Agent_model.py:266\u001b[0m, in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mvstack([e\u001b[39m.\u001b[39maction \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    265\u001b[0m rewards \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mvstack([e\u001b[39m.\u001b[39mreward \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m--> 266\u001b[0m next_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mvstack([e\u001b[39m.\u001b[39mnext_state \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    267\u001b[0m dones \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mvstack([e\u001b[39m.\u001b[39mdone \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m])\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    269\u001b[0m \u001b[39mreturn\u001b[39;00m (states, actions, rewards, next_states, dones)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from DDPG_Agent_model import DDPG_Agent\n",
    "env = gym.make('MountainCarContinuous-v0',render_mode='rgb_array')\n",
    "def ddpg(n_episodes=1000, max_t=700, print_every=20):\n",
    "    scores_deque = deque(maxlen=print_every) # last 100 scores\n",
    "    scores = [] # list containing scores from each episode\n",
    "\n",
    "    # For each episode\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()[0] # Reset the environment\n",
    "        agent.reset() # Reset the agent\n",
    "        score = 0 # Initialize the score\n",
    "        min_action = 0.5\n",
    "        #img = plt.imshow(env.render())\n",
    "        # For each time step\n",
    "        for t in range(max_t):\n",
    "            #img.set_data(env.render())\n",
    "            #plt.axis('off')\n",
    "            #display(plt.gcf())\n",
    "            #display(wait=True)\n",
    "            \n",
    "\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state # Roll over the state to next time step\n",
    "            score += reward # Update the score\n",
    "\n",
    "            if min_action > action:\n",
    "                min_action = action\n",
    "\n",
    "            #if t % 10 == 0:\n",
    "                #print(\"Action:\",action[0], \"State:\",state[0], \"Reward:\",reward, \"Score:\",score)\n",
    "                #clear_output(wait=True)\n",
    "            if done:\n",
    "                break\n",
    "            #clear_output(wait=True)\n",
    "        scores_deque.append(score) # Save most recent score to the deque\n",
    "        scores.append(score) # Save most recent score to the list\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMin Action: {}'.format(i_episode, np.mean(scores_deque), min_action), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=90.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "# Plotting the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1,len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
